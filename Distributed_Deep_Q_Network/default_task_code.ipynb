{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Deep Q-Learning \n",
    "\n",
    "The goal of this assignment is to implement and experiment with both single-core and distributed versions of the deep reinforcement learning algorithm Deep Q Networks (DQN). In particular, DQN will be run in the classic RL benchmark Cart-Pole and abblation experiments will be run to observe the impact of the different DQN components. \n",
    "\n",
    "The relevant content about DQN can be found Q-Learning and SARSA are in the following course notes from CS533.\n",
    "\n",
    "https://oregonstate.instructure.com/courses/1719746/files/75047394/download?wrap=1\n",
    "\n",
    "The full pseudo-code for DQN is on slide 45 with prior slides introducing the individual components. \n",
    "\n",
    "\n",
    "## Recap of DQN \n",
    "\n",
    "From the course slides it can be seen that DQN is simply the standard table-based Q-learning algorithm but with three extensions:\n",
    "\n",
    "1) Use of function approximation via a neural network instead of a Q-table. \n",
    "2) Use of experience replay. \n",
    "3) Use of a target network. \n",
    "\n",
    "Extension (1) allows for scaling to problems with enormous state spaces, such as when the states correspond to images or sequences of images. Extensions (2) and (3) are claimed to improve the robustness and effectiveness of DQN compared. \n",
    "\n",
    "(2) adjusts Q-learning so that updates are not just performed on individual experiences as they arrive. But rather, experiences are stored in a memory buffer and updates are performed by sampling random mini-batches of experience tuples from the memory buffer and updating the network based on the mini-batch. This allows for reuse of experience as well as helping to reduce correlation between successive updates, which is claimed to be beneficial. \n",
    "\n",
    "(3) adjusts the way that target values are computed for the Q-learning updates. Let $Q_{\\theta}(s,a)$ be the function approximation network with parameters $\\theta$ for representing the Q-function. Given an experience tuple $(s, a, r, s')$ the origional Q-learning algorithm updates the parameters so that $Q_{\\theta}(s,a)$ moves closer to the target value: \n",
    "\\begin{equation}\n",
    "r + \\beta \\max_a' Q_{\\theta}(s',a') \n",
    "\\end{equation}\n",
    "Rather, DQN stores two function approximation networks. The first is the update network with parameters $\\theta$, which is the network that is continually updated during learning. The second is a target network with parameters $\\theta'$. Given the same experience tuple, DQN will update the parameters $\\theta$ so that $Q_{\\theta}(s,a)$ moves toward a target value based on the target network:\n",
    "\\begin{equation}\n",
    "r + \\beta \\max_a' Q_{\\theta'}(s',a') \n",
    "\\end{equation}\n",
    "Periodically the target network is updated with the most recent parameters $\\theta' \\leftarrow \\theta$. This use of a target network is claimed to stabilize learning.\n",
    "\n",
    "In the assignment you will get to see an example of the impact of both the target network and experience replay.\n",
    "\n",
    "Further reading about DQN and its application to learning to play Atari games can be found in the following paper. \n",
    "\n",
    "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G. and Petersen, S., 2015. Human-level control through deep reinforcement learning. Nature, 518(7540), p.529.\n",
    "https://oregonstate.instructure.com/courses/1719746/files/75234294/download?wrap=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --user gym[Box2D]\n",
    "!pip3 install --user torch\n",
    "!pip3 install --user JSAnimation\n",
    "!pip3 install --user matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the packages for enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import ray\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from random import uniform, randint\n",
    "\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "from dqn_model import DQNModel\n",
    "from dqn_model import _DQNModel\n",
    "from memory import ReplayBuffer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "FloatTensor = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful PyTorch functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "This assignment will use the PyTorch library for the required neural network functionality. You do not need to be familiar with the details of PyTorch or neural network training. However, the assignment will require dealing with data in the form of tensors.  \n",
    "\n",
    "The mini-batches used to train the PyTorch neural network is expected to be represented as a tensor matrix. The function `FloatTensor` can convert a list or NumPy matrix into a tensor matrix if needed. \n",
    "\n",
    "You can find more infomation here: https://pytorch.org/docs/stable/tensors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list\n",
    "m = [[3,2,1],[6,4,5],[7,8,9]]\n",
    "print(m)\n",
    "\n",
    "# tensor matrix\n",
    "m_tensor = FloatTensor(m)\n",
    "print(type(m_tensor))\n",
    "print(m_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor.max()\n",
    "Once you have a tenosr maxtrix, you can use torch.max(m_tensor, dim) to get the max value and max index corresponding to the dimension you choose.\n",
    "```\n",
    ">>> a = torch.randn(4, 4)\n",
    ">>> a\n",
    "tensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n",
    "        [ 1.1949, -1.1127, -2.2379, -0.6702],\n",
    "        [ 1.5717, -0.9207,  0.1297, -1.8768],\n",
    "        [-0.6172,  1.0036, -0.6060, -0.2432]])\n",
    ">>> torch.max(a, 1)\n",
    "torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))\n",
    "```\n",
    "You can find more infomation here: https://pytorch.org/docs/stable/torch.html#torch.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value, index = torch.max(m_tensor, dim = 1)\n",
    "print(max_value, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Environment\n",
    "### CartPole-v0:  \n",
    "CartPole is a classic control task that is often used as an introductory reinforcement learning benchmark. The environment involves controlling a 2d cart that can move in either the left or right direction on a frictionless track. A pole is attached to the cart via an unactuated joint. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.  \n",
    "(You can find more infomation by this Link: https://gym.openai.com/envs/CartPole-v0/)  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Env name and action space for CartPole\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "# Move left, Move right\n",
    "ACTION_DICT = {\n",
    "    \"LEFT\": 0,\n",
    "    \"RIGHT\":1\n",
    "}\n",
    "# Register the environment\n",
    "env_CartPole = gym.make(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set result saveing floder\n",
    "result_floder = ENV_NAME\n",
    "result_file = ENV_NAME + \"/results.txt\"\n",
    "if not os.path.isdir(result_floder):\n",
    "    os.mkdir(result_floder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function\n",
    "Plot results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(total_rewards ,learning_num, legend):\n",
    "    print(\"\\nLearning Performance:\\n\")\n",
    "    episodes = []\n",
    "    for i in range(len(total_rewards)):\n",
    "        episodes.append(i * learning_num + 1)\n",
    "        \n",
    "    plt.figure(num = 1)\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(episodes, total_rewards)\n",
    "    plt.title('performance')\n",
    "    plt.legend(legend)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"total rewards\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams\n",
    "When function approximation is involves, especially neural networks, additional hyper parameters are inroduced and setting the parameters can require experience. Below is a list of the hyperparameters used in this assignment and values for the parameters that have worked well for a basic DQN implementation. You will adjust these values for particular parts of the assignment. For example, experiments that do not use the target network will set 'use_target_model' to False. \n",
    "\n",
    "You can find the more infomation about these hyperparameters in the notation of DQN_agent.init() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_CartPole = {\n",
    "    'epsilon_decay_steps' : 100000, \n",
    "    'final_epsilon' : 0.1,\n",
    "    'batch_size' : 32, \n",
    "    'update_steps' : 10, \n",
    "    'memory_size' : 2000, \n",
    "    'beta' : 0.99, \n",
    "    'model_replace_freq' : 2000,\n",
    "    'learning_rate' : 0.0003,\n",
    "    'use_target_model': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Part 1: Non-distributed DQN\n",
    "\n",
    "In this part, you will complete an implementation of DQN and run experiments on the CartPole environment from OpenAI Gym.  \n",
    "Note that OpenAI Gym has many other environments that use the same interface---so this experience will allow the curious student to easily explore these algorithms more widely. \n",
    "\n",
    "Below you need to fill in the missing code for the DQN implementation. \n",
    "\n",
    "The Run function below can then be used to generate learning curves. \n",
    "\n",
    "You should conduct the following experiments involving different features of DQN. \n",
    "\n",
    "1. DQN without a replay buffer and without a target network. This is just standard Q-learning with a function approximator.\n",
    "    The corresponding parameters are: memory_size = 1, update_steps = 1, batch_size = 1, use_target_model = False  \n",
    "    \n",
    "2. DQN without a replay buffer (but including the target network).   \n",
    "    The corresponding parameters are: memory_size = 1, update_steps = 1, batch_size = 1, use_target_model = True  \n",
    "\n",
    "3. DQN with a replay buffer, but without a target network.   \n",
    "    Here you set use_target_model = False and otherwise set the replay memory parameters to the above suggested values \n",
    "   \n",
    "4. Full DQN\n",
    "\n",
    "For each experiment, record the parameters that you used, plot the resulting learning curves, and give a summary of your observations regarding the differences you observed. \n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## DQN Agent\n",
    "\n",
    "The full DQN agent involves a number of functions, the neural network, and the replay memory. Interfaces to a neural network model and memory are provided. \n",
    "\n",
    "Some useful information is below:   \n",
    "- Neural Network Model: The network is used to represent the Q-function $Q(s,a)$. It takes a state $s$ as input and returns a vector of Q-values, one value for each action. The following interface functions are used for predicting Q-values, actions, and updating the neural network model parameters. \n",
    "    1. Model.predict(state) --- Returns the action that has the best Q-value in 'state'.\n",
    "    2. Model.predict_batch(states) --- This is used to predict both the Q-values and best actions for a batch of states. Given a batch of states, the function returns: 1) 'best_actions' a vector containing the best action for each input state, and 2) 'q_values' a matrix where each row gives the Q-value for all actions of each state (one row per state).   \n",
    "    3. Model.fit(q_values, q_target) --- It is used to update the neural network (via back-propagation). 'q_values' is a vector containing the Q-value predictions for a list of state-action pairs (e.g. from a batch of experience tuples). 'q_target' is a vector containing target values that we would like the correspoinding predictions to get closer to. This function updates the network in a way that the network predictions will ideally be closer to the targets. There is no return value.  \n",
    "    4. Model.replace(another_model) --- It takes another model as input, and replace the weight of itself by the input model.\n",
    "- Memory: This is the buffer used to store experience tuples for experience replay.\n",
    "    1. Memory.add(state, action, reward, state', is_terminal) --- It takes one example as input, and store it into its storage.  \n",
    "    2. Memory.sample(batch_size) --- It takes a batch_size int number as input. Return 'batch_size' number of randomly selected examples from the current memory buffer. The batch takes the form (states, actions, rewards, states', is_terminals) with each component being a vector/list of size equal to batch_size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_agent(object):\n",
    "    def __init__(self, env, hyper_params, action_space = len(ACTION_DICT)):\n",
    "        \n",
    "        self.env = env\n",
    "        self.max_episode_steps = env._max_episode_steps\n",
    "        \n",
    "        \"\"\"\n",
    "            beta: The discounted factor of Q-value function\n",
    "            (epsilon): The explore or exploit policy epsilon. \n",
    "            initial_epsilon: When the 'steps' is 0, the epsilon is initial_epsilon, 1\n",
    "            final_epsilon: After the number of 'steps' reach 'epsilon_decay_steps', \n",
    "                The epsilon set to the 'final_epsilon' determinately.\n",
    "            epsilon_decay_steps: The epsilon will decrease linearly along with the steps from 0 to 'epsilon_decay_steps'.\n",
    "        \"\"\"\n",
    "        self.beta = hyper_params['beta']\n",
    "        self.initial_epsilon = 1\n",
    "        self.final_epsilon = hyper_params['final_epsilon']\n",
    "        self.epsilon_decay_steps = hyper_params['epsilon_decay_steps']\n",
    "\n",
    "        \"\"\"\n",
    "            episode: Record training episode\n",
    "            steps: Add 1 when predicting an action\n",
    "            learning: The trigger of agent learning. It is on while training agent. It is off while testing agent.\n",
    "            action_space: The action space of the current environment, e.g 2.\n",
    "        \"\"\"\n",
    "        self.episode = 0\n",
    "        self.steps = 0\n",
    "        self.best_reward = 0\n",
    "        self.learning = True\n",
    "        self.action_space = action_space\n",
    "\n",
    "        \"\"\"\n",
    "            input_len The input length of the neural network. It equals to the length of the state vector.\n",
    "            output_len: The output length of the neural network. It is equal to the action space.\n",
    "            eval_model: The model for predicting action for the agent.\n",
    "            target_model: The model for calculating Q-value of next_state to update 'eval_model'.\n",
    "            use_target_model: Trigger for turn 'target_model' on/off\n",
    "        \"\"\"\n",
    "        state = env.reset()\n",
    "        input_len = len(state)\n",
    "        output_len = action_space\n",
    "        self.eval_model = DQNModel(input_len, output_len, learning_rate = hyper_params['learning_rate'])\n",
    "        self.use_target_model = hyper_params['use_target_model']\n",
    "        if self.use_target_model:\n",
    "            self.target_model = DQNModel(input_len, output_len)\n",
    "#         memory: Store and sample experience replay.\n",
    "        self.memory = ReplayBuffer(hyper_params['memory_size'])\n",
    "        \n",
    "        \"\"\"\n",
    "            batch_size: Mini batch size for training model.\n",
    "            update_steps: The frequence of traning model\n",
    "            model_replace_freq: The frequence of replacing 'target_model' by 'eval_model'\n",
    "        \"\"\"\n",
    "        self.batch_size = hyper_params['batch_size']\n",
    "        self.update_steps = hyper_params['update_steps']\n",
    "        self.model_replace_freq = hyper_params['model_replace_freq']\n",
    "        \n",
    "    # Linear decrease function for epsilon\n",
    "    def linear_decrease(self, initial_value, final_value, curr_steps, final_decay_steps):\n",
    "        decay_rate = curr_steps / final_decay_steps\n",
    "        if decay_rate > 1:\n",
    "            decay_rate = 1\n",
    "        return initial_value - (initial_value - final_value) * decay_rate\n",
    "    \n",
    "    def explore_or_exploit_policy(self, state):\n",
    "        p = uniform(0, 1)\n",
    "        # Get decreased epsilon\n",
    "        epsilon = self.linear_decrease(self.initial_epsilon, \n",
    "                               self.final_epsilon,\n",
    "                               self.steps,\n",
    "                               self.epsilon_decay_steps)\n",
    "        \n",
    "        if p < epsilon:\n",
    "            #return action\n",
    "            return randint(0, self.action_space - 1)\n",
    "        else:\n",
    "            #return action\n",
    "            return self.greedy_policy(state)\n",
    "        \n",
    "    def greedy_policy(self, state):\n",
    "        return self.eval_model.predict(state)\n",
    "    \n",
    "    # This next function will be called in the main RL loop to update the neural network model given a batch of experience\n",
    "    # 1) Sample a 'batch_size' batch of experiences from the memory.\n",
    "    # 2) Predict the Q-value from the 'eval_model' based on (states, actions)\n",
    "    # 3) Predict the Q-value from the 'target_model' base on (next_states), and take the max of each Q-value vector, Q_max\n",
    "    # 4) If is_terminal == 1, q_target = reward + discounted factor * Q_max, otherwise, q_target = reward\n",
    "    # 5) Call fit() to do the back-propagation for 'eval_model'.\n",
    "    def update_batch(self):\n",
    "        if len(self.memory) < self.batch_size or self.steps % self.update_steps != 0:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "\n",
    "        (states, actions, reward, next_states,\n",
    "         is_terminal) = batch\n",
    "        \n",
    "        states = states\n",
    "        next_states = next_states\n",
    "        terminal = FloatTensor([1 if t else 0 for t in is_terminal])\n",
    "        reward = FloatTensor(reward)\n",
    "        batch_index = torch.arange(self.batch_size,\n",
    "                                   dtype=torch.long)\n",
    "        \n",
    "        # Current Q Values\n",
    "        _, q_values = self.eval_model.predict_batch(states)\n",
    "        q_values = q_values[batch_index, actions]\n",
    "        \n",
    "        # Calculate target\n",
    "        if self.use_target_model:\n",
    "            actions, q_next = self.target_model.predict_batch(next_states)#------\n",
    "        else:\n",
    "            actions, q_next = self.eval_model.predict_batch(next_states)#------\n",
    "            \n",
    "        #INSERT YOUR CODE HERE --- neet to compute 'q_targets' used below\n",
    "        \n",
    "        # update model\n",
    "        self.eval_model.fit(q_values, q_target)\n",
    "    \n",
    "    def learn_and_evaluate(self, training_episodes, test_interval):\n",
    "        test_number = training_episodes // test_interval\n",
    "        all_results = []\n",
    "        \n",
    "        for i in range(test_number):\n",
    "            # learn\n",
    "            self.learn(test_interval)\n",
    "            \n",
    "            # evaluate\n",
    "            avg_reward = self.evaluate()\n",
    "            all_results.append(avg_reward)\n",
    "            \n",
    "        return all_results\n",
    "    \n",
    "    def learn(self, test_interval):\n",
    "        for episode in tqdm(range(test_interval), desc=\"Training\"):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            steps = 0\n",
    "\n",
    "            while steps < self.max_episode_steps and not done:\n",
    "                #INSERT YOUR CODE HERE\n",
    "                # add experience from explore-exploit policy to memory\n",
    "                # update the model every 'update_steps' of experience\n",
    "                # update the target network (if the target network is being used) every 'model_replace_freq' of experiences \n",
    "                \n",
    "    def evaluate(self, trials = 30):\n",
    "        total_reward = 0\n",
    "        for _ in tqdm(range(trials), desc=\"Evaluating\"):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            steps = 0\n",
    "\n",
    "            while steps < self.max_episode_steps and not done:\n",
    "                steps += 1\n",
    "                action = self.greedy_policy(state)\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "        avg_reward = total_reward / trials\n",
    "        print(avg_reward)\n",
    "        f = open(result_file, \"a+\")\n",
    "        f.write(str(avg_reward) + \"\\n\")\n",
    "        f.close()\n",
    "        if avg_reward >= self.best_reward:\n",
    "            self.best_reward = avg_reward\n",
    "            self.save_model()\n",
    "        return avg_reward\n",
    "\n",
    "    # save model\n",
    "    def save_model(self):\n",
    "        self.eval_model.save(result_floder + '/best_model.pt')\n",
    "        \n",
    "    # load model\n",
    "    def load_model(self):\n",
    "        self.eval_model.load(result_floder + '/best_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_episodes, test_interval = 10000, 50\n",
    "agent = DQN_agent(env_CartPole, hyperparams_CartPole)\n",
    "result = agent.learn_and_evaluate(training_episodes, test_interval)\n",
    "plot_result(result, test_interval, [\"batch_update with target_model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Part 2: Distributed DQN\n",
    "***\n",
    "\n",
    "Here you will implement a distributed version of the above DQN approach. The distribution approach can be the same as that used for the table-based distribution Q-learning algorithm from homework 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init(include_webui=False, ignore_reinit_error=True, redis_max_memory=500000000, object_store_memory=5000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed DQN agent\n",
    "The idea is to speedup learning by creating actors to collect data and a model_server to update the neural network model.\n",
    "- Collector: There is a simulator inside each collector. Their job is to collect exprience from the simulator, and send them to the memory server. They follow the explore_or_exploit policy, getting greedy action from model server. Also, call update function of model server to update the model.  \n",
    "- Evaluator: There is a simulator inside the evaluator. It is called by the the Model Server, taking eval_model from it, and test its performance.\n",
    "- Model Server: Stores the evalation and target networks. It Takes experiences from Memory Server and updates the Q-network, also replacing target Q-network periodically. It also interfaces to the evaluator periodically. \n",
    "- Memory Server: It is used to store/sample experience relays.\n",
    "\n",
    "An image of this architecture is below. \n",
    "\n",
    "For this part, you should use our custom_cartpole as your enviroment. This version of cartpole is slower, which allows for the benefits of distributed experience collection to be observed. In particular, the time to generate an experience tuple needs to be non-trivial compared to the time needed to do a neural network model update. \n",
    "\n",
    "<span style=\"color:green\">It is better to run the distributed DQN agent in exclusive node, not in Jupyter notebook</span>\n",
    "```\n",
    "Store all of your distrited DQN code into a python file.\n",
    "ssh colfax (get access to the Devcloud on terminal)\n",
    "qsub -I -lselect=1\n",
    "python3 distributed_dqn.py\n",
    "```\n",
    "\n",
    "<img src=\"distributed DQN.png\">\n",
    "\n",
    "For this part of the homework you need to submit your code for distributed DQN and run experiments that vary the number of workers involved. Produce some learning curves and timing results and discuss your observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_remote import ReplayBuffer_remote\n",
    "from dqn_model import _DQNModel\n",
    "import torch\n",
    "from custom_cartpole import CartPoleEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Env name and action space for CartPole\n",
    "ENV_NAME = 'CartPole_distributed'\n",
    "\n",
    "# Set result saveing floder\n",
    "result_floder = ENV_NAME + \"_distributed\"\n",
    "result_file = ENV_NAME + \"/results.txt\"\n",
    "if not os.path.isdir(result_floder):\n",
    "    os.mkdir(result_floder)\n",
    "torch.set_num_threads(12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel, 2019 update 1)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_2019u1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
